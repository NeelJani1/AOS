import copy
import logging
import os
import pickle
import random
import time

import numpy as np
import torch
import torchvision.transforms as transforms
import torchvision.utils as tvu
import tqdm
from datasets import (
    all_but_one_class_path_dataset,
    data_transform,
    get_dataset,
    get_forget_dataset,
    inverse_data_transform,
)
from functions import create_class_labels, cycle, get_optimizer
from functions.denoising import generalized_steps_conditional
from functions.losses import loss_registry_conditional
from models.diffusion import Conditional_Model
from models.ema import EMAHelper
from torch.utils.data import DataLoader
from torchvision.datasets import ImageFolder


def torch2hwcuint8(x, clip=False):
    if clip:
        x = torch.clamp(x, -1, 1)
    x = (x + 1.0) / 2.0
    return x


def get_beta_schedule(beta_schedule, *, beta_start, beta_end, num_diffusion_timesteps):
    def sigmoid(x):
        return 1 / (np.exp(-x) + 1)

    if beta_schedule == "quad":
        betas = (
            np.linspace(
                beta_start**0.5,
                beta_end**0.5,
                num_diffusion_timesteps,
                dtype=np.float64,
            )
            ** 2
        )
    elif beta_schedule == "linear":
        betas = np.linspace(
            beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64
        )
    elif beta_schedule == "const":
        betas = beta_end * np.ones(num_diffusion_timesteps, dtype=np.float64)
    elif beta_schedule == "jsd":
        betas = 1.0 / np.linspace(
            num_diffusion_timesteps, 1, num_diffusion_timesteps, dtype=np.float64
        )
    elif beta_schedule == "sigmoid":
        betas = np.linspace(-6, 6, num_diffusion_timesteps)
        betas = sigmoid(betas) * (beta_end - beta_start) + beta_start
    else:
        raise NotImplementedError(beta_schedule)
    assert betas.shape == (num_diffusion_timesteps,)
    return betas


class Diffusion(object):
    def __init__(self, args, config):
        self.args = args
        self.config = config

        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        self.model_var_type = config.model.var_type
        betas = get_beta_schedule(
            beta_schedule=config.diffusion.beta_schedule,
            beta_start=config.diffusion.beta_start,
            beta_end=config.diffusion.beta_end,
            num_diffusion_timesteps=config.diffusion.num_diffusion_timesteps,
        )
        betas = self.betas = torch.from_numpy(betas).float().to(self.device)
        self.num_timesteps = betas.shape[0]

        alphas = 1.0 - betas
        alphas_cumprod = alphas.cumprod(dim=0)
        alphas_cumprod_prev = torch.cat(
            [torch.ones(1).to(self.device), alphas_cumprod[:-1]], dim=0
        )
        posterior_variance = (
            betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)
        )
        if self.model_var_type == "fixedlarge":
            self.logvar = betas.log()
        elif self.model_var_type == "fixedsmall":
            self.logvar = posterior_variance.clamp(min=1e-20).log()

    def _clean_state_dict(self, state_dict):
        """Clean state dict by removing module and compile prefixes"""
        cleaned_dict = {}
        for k, v in state_dict.items():
            # Remove all possible prefixes
            k_clean = k.replace('module.', '').replace('_orig_mod.', '')
            cleaned_dict[k_clean] = v
        return cleaned_dict

    def _setup_model_optimized(self, config, resume_from=None):
        """Optimized model setup with proper compilation order"""
        model = Conditional_Model(config)
        model.to(self.device)
        
        # Apply DataParallel first, then compile
        model = torch.nn.DataParallel(model)
        model = torch.compile(model)
        
        optimizer = get_optimizer(config, model.parameters())
        
        start_step = 0
        ema_helper = None
        
        if resume_from and os.path.exists(resume_from):
            try:
                logging.info(f"Loading checkpoint from {resume_from}")
                states = torch.load(resume_from, map_location=self.device)
                
                # ===== FIXED CHECKPOINT LOADING =====
                # Step 1: Clean all prefixes from checkpoint
                cleaned_dict = {}
                for k, v in states[0].items():
                    # Remove torch.compile prefix (_orig_mod.)
                    k_clean = k.replace('_orig_mod.', '')
                    # Remove DataParallel prefix (module.)
                    k_clean = k_clean.replace('module.', '')
                    cleaned_dict[k_clean] = v
                
                # Step 2: Add module. prefix back for current DataParallel model
                state_dict_to_load = {}
                for k, v in cleaned_dict.items():
                    if not k.startswith('module.'):
                        state_dict_to_load[f'module.{k}'] = v
                    else:
                        state_dict_to_load[k] = v
                
                # Step 3: Load weights
                missing_keys, unexpected_keys = model.load_state_dict(state_dict_to_load, strict=False)
                
                # Step 4: Verify loading success
                if len(missing_keys) == 0 and len(unexpected_keys) == 0:
                    logging.info("✅ All checkpoint weights loaded successfully!")
                else:
                    logging.warning(f"⚠️ Keys mismatch - Missing: {len(missing_keys)}, Unexpected: {len(unexpected_keys)}")
                    if len(missing_keys) <= 5:
                        logging.warning(f"Missing keys: {missing_keys}")
                    if len(unexpected_keys) <= 5:
                        logging.warning(f"Unexpected keys: {unexpected_keys}")
                # ===== END FIXED CHECKPOINT LOADING =====
                
                # Load optimizer state (keep your existing code below)
                if len(states) > 1 and states[1] is not None:
                    try:
                        optimizer.load_state_dict(states[1])
                        # Move optimizer state to GPU
                        for state in optimizer.state.values():
                            for k, v in state.items():
                                if isinstance(v, torch.Tensor):
                                    state[k] = v.to(self.device)
                    except Exception as e:
                        logging.warning(f"Failed to load optimizer state: {e}")
                
                start_step = states[2] if len(states) > 2 else 0
                logging.info(f"Successfully resumed from step {start_step}")
                
                # Setup EMA AFTER loading model weights (keep your existing EMA code)
                if config.model.ema:
                    ema_helper = EMAHelper(mu=config.model.ema_rate)
                    ema_helper.register(model)
                    if len(states) > 3:
                        try:
                            ema_helper.load_state_dict(states[3])
                            logging.info("Loaded EMA state")
                        except Exception as e:
                            logging.warning(f"Failed to load EMA state: {e}")
                            logging.info("Re-initializing EMA from current model state")
                            
            except Exception as e:
                logging.error(f"Checkpoint loading failed: {e}")
                import traceback
                traceback.print_exc()
                start_step = 0

        else:
            logging.info("No checkpoint found, starting from scratch")
        
        # Initialize EMA if not already done and config requires it
        if config.model.ema and ema_helper is None:
            ema_helper = EMAHelper(mu=config.model.ema_rate)
            ema_helper.register(model)
        
        return model, optimizer, ema_helper, start_step

    def save_fim(self):
        args, config = self.args, self.config
        bs = torch.cuda.device_count()
        fim_dataset = ImageFolder(
            os.path.join(args.ckpt_folder, "class_samples"),
            transform=transforms.ToTensor(),
        )
        fim_loader = DataLoader(
            fim_dataset,
            batch_size=bs,
            num_workers=config.data.num_workers,
            shuffle=True,
        )

        print("Loading checkpoints {}".format(args.ckpt_folder))
        model = Conditional_Model(config)
        states = torch.load(
            os.path.join(self.args.ckpt_folder, "ckpts/ckpt.pth"),
            map_location=self.device,
        )
        model = model.to(self.device)
        model = torch.nn.DataParallel(model)
        
        # Use cleaned state dict
        cleaned_state_dict = self._clean_state_dict(states[0])
        model.load_state_dict(cleaned_state_dict, strict=True)
        model.eval()

        fisher_dict = {}
        fisher_dict_temp_list = [{} for _ in range(bs)]

        for name, param in model.named_parameters():
            fisher_dict[name] = param.data.clone().zero_()
            for i in range(bs):
                fisher_dict_temp_list[i][name] = param.data.clone().zero_()

        for step, data in enumerate(
            tqdm.tqdm(fim_loader, desc="Calculating Fisher information matrix")
        ):
            x, c = data
            x, c = x.to(self.device), c.to(self.device)

            b = self.betas
            ts = torch.chunk(torch.arange(0, self.num_timesteps), args.n_chunks)

            for _t in ts:
                for i in range(len(_t)):
                    e = torch.randn_like(x)
                    t = torch.tensor([_t[i]]).expand(bs).to(self.device)

                    if i == 0:
                        loss = loss_registry_conditional[config.model.type](
                            model, x, t, c, e, b, keepdim=True
                        )
                    else:
                        loss += loss_registry_conditional[config.model.type](
                            model, x, t, c, e, b, keepdim=True
                        )

                for i in range(bs):
                    model.zero_grad()
                    if i != len(loss) - 1:
                        loss[i].backward(retain_graph=True)
                    else:
                        loss[i].backward()
                    for name, param in model.named_parameters():
                        fisher_dict_temp_list[i][name] += param.grad.data
                del loss

            for name, param in model.named_parameters():
                for i in range(bs):
                    fisher_dict[name].data += (
                        fisher_dict_temp_list[i][name].data ** 2
                    ) / len(fim_loader.dataset)
                    fisher_dict_temp_list[i][name] = (
                        fisher_dict_temp_list[i][name].clone().zero_()
                    )

            if (step + 1) % config.training.save_freq == 0:
                with open(os.path.join(args.ckpt_folder, "fisher_dict.pkl"), "wb") as f:
                    pickle.dump(fisher_dict, f)

        with open(os.path.join(args.ckpt_folder, "fisher_dict.pkl"), "wb") as f:
            pickle.dump(fisher_dict, f)

    def train(self):
        args, config = self.args, self.config
        D_train_loader = get_dataset(args, config)
        D_train_iter = cycle(D_train_loader)
        
        # Use optimized model setup
        ckpt_path = os.path.join(self.config.ckpt_dir, "ckpt.pth")
        model, optimizer, ema_helper, start_step = self._setup_model_optimized(config, ckpt_path)
        
        model.train()
        start_time = time.time()
        
        for step in range(start_step, self.config.training.n_iters):
            model.train()
            x, c = next(D_train_iter)
            n = x.size(0)
            x = x.to(self.device)
            x = data_transform(self.config, x)
            e = torch.randn_like(x)
            b = self.betas

            # antithetic sampling
            t = torch.randint(
                low=0, high=self.num_timesteps, size=(n // 2 + 1,)
            ).to(self.device)
            t = torch.cat([t, self.num_timesteps - t - 1], dim=0)[:n]
            loss = loss_registry_conditional[config.model.type](model, x, t, c, e, b)
            
            if (step + 1) % self.config.training.log_freq == 0 and step > start_step:
                end_time = time.time()
                step_time = end_time - start_time
                logging.info(f"step: {step + 1}, loss: {loss.item():.3f}, time: {step_time:.2f}s")
                start_time = time.time()
                
            optimizer.zero_grad()
            loss.backward()

            try:
                torch.nn.utils.clip_grad_norm_(
                    model.parameters(), config.optim.grad_clip
                )
            except Exception:
                pass
            optimizer.step()

            if ema_helper:
                ema_helper.update(model)

            if (step + 1) % self.config.training.snapshot_freq == 0 or (step + 1) == self.config.training.n_iters:
                logging.info(f"Saving checkpoint at step {step + 1}")
                
                states = [
                    model.state_dict(),
                    optimizer.state_dict(),
                    step + 1,
                ]
                if ema_helper:
                    states.append(ema_helper.state_dict())

                torch.save(
                    states,
                    os.path.join(self.config.ckpt_dir, "ckpt.pth"),
                )
                
                # Also save a backup every 5000 steps
                if (step + 1) % 5000 == 0:
                    torch.save(
                        states,
                        os.path.join(self.config.ckpt_dir, f"ckpt_{step + 1}.pth"),
                    )
                
                logging.info(f"Checkpoint saved at step {step + 1}")

    def train_forget(self):
        args, config = self.args, self.config
        logging.info(f"Training diffusion forget with contrastive and EWC")

        D_train_loader = all_but_one_class_path_dataset(
            config,
            os.path.join(args.ckpt_folder, "class_samples"),
            args.label_to_forget,
        )
        D_train_iter = cycle(D_train_loader)

        print("Loading checkpoints {}".format(args.ckpt_folder))
        
        # Use optimized model setup
        ckpt_path = os.path.join(args.ckpt_folder, "ckpts/ckpt.pth")
        model, optimizer, ema_helper, _ = self._setup_model_optimized(config, ckpt_path)

        with open(os.path.join(args.ckpt_folder, "fisher_dict.pkl"), "rb") as f:
            fisher_dict = pickle.load(f)

        params_mle_dict = {}
        for name, param in model.named_parameters():
            params_mle_dict[name] = param.data.clone()

        label_choices = list(range(config.data.n_classes))
        label_choices.remove(args.label_to_forget)

        for step in range(0, config.training.n_iters):
            model.train()
            x_remember, c_remember = next(D_train_iter)
            x_remember, c_remember = x_remember.to(self.device), c_remember.to(self.device)
            x_remember = data_transform(config, x_remember)

            n = x_remember.size(0)
            channels = config.data.channels
            img_size = config.data.image_size
            c_forget = (torch.ones(n, dtype=int) * args.label_to_forget).to(self.device)
            x_forget = (torch.rand((n, channels, img_size, img_size), device=self.device) - 0.5) * 2.0
            e_remember = torch.randn_like(x_remember)
            e_forget = torch.randn_like(x_forget)
            b = self.betas

            t = torch.randint(low=0, high=self.num_timesteps, size=(n // 2 + 1,)).to(self.device)
            t = torch.cat([t, self.num_timesteps - t - 1], dim=0)[:n]
            loss = loss_registry_conditional[config.model.type](
                model, x_forget, t, c_forget, e_forget, b, cond_drop_prob=0.0
            ) + config.training.gamma * loss_registry_conditional[config.model.type](
                model, x_remember, t, c_remember, e_remember, b, cond_drop_prob=0.0
            )

            ewc_loss = 0.0
            for name, param in model.named_parameters():
                _loss = fisher_dict[name].to(self.device) * (param - params_mle_dict[name].to(self.device)) ** 2
                loss += config.training.lmbda * _loss.sum()
                ewc_loss += config.training.lmbda * _loss.sum()

            if (step + 1) % config.training.log_freq == 0:
                logging.info(f"step: {step + 1}, loss: {loss.item():.3f}, ewc_loss: {ewc_loss.item():.3f}")

            optimizer.zero_grad()
            loss.backward()

            try:
                torch.nn.utils.clip_grad_norm_(model.parameters(), config.optim.grad_clip)
            except Exception:
                pass

            optimizer.step()

            if ema_helper:
                ema_helper.update(model)

            if (step + 1) % config.training.snapshot_freq == 0:
                states = [model.state_dict(), optimizer.state_dict(), step + 1]
                if ema_helper:
                    states.append(ema_helper.state_dict())

                torch.save(states, os.path.join(config.ckpt_dir, "ckpt.pth"))

    def retrain(self):
        args, config = self.args, self.config

        D_remain_loader, _ = get_forget_dataset(args, config, args.label_to_forget)
        D_remain_iter = cycle(D_remain_loader)

        # Use optimized model setup without resume
        model, optimizer, ema_helper, _ = self._setup_model_optimized(config)

        model.train()
        start_time = time.time()
        
        for step in range(0, self.config.training.n_iters):
            model.train()
            x, c = next(D_remain_iter)

            n = x.size(0)
            x = x.to(self.device)
            x = data_transform(self.config, x)
            e = torch.randn_like(x)
            b = self.betas

            t = torch.randint(low=0, high=self.num_timesteps, size=(n // 2 + 1,)).to(self.device)
            t = torch.cat([t, self.num_timesteps - t - 1], dim=0)[:n]
            loss = loss_registry_conditional[config.model.type](model, x, t, c, e, b)

            if (step + 1) % self.config.training.log_freq == 0:
                end_time = time.time()
                step_time = end_time - start_time
                logging.info(f"step: {step + 1}, loss: {loss.item():.3f}, time: {step_time:.2f}s")
                start_time = time.time()

            optimizer.zero_grad()
            loss.backward()

            try:
                torch.nn.utils.clip_grad_norm_(model.parameters(), config.optim.grad_clip)
            except Exception:
                pass
            optimizer.step()

            if ema_helper:
                ema_helper.update(model)

            if (step + 1) % self.config.training.snapshot_freq == 0:
                states = [model.state_dict(), optimizer.state_dict(), step + 1]
                if ema_helper:
                    states.append(ema_helper.state_dict())

                torch.save(states, os.path.join(self.config.ckpt_dir, "ckpt.pth"))

    def saliency_unlearn(self):
        args, config = self.args, self.config

        D_remain_loader, D_forget_loader = get_forget_dataset(args, config, args.label_to_forget)
        D_remain_iter = cycle(D_remain_loader)
        D_forget_iter = cycle(D_forget_loader)

        if args.mask_path:
            mask = torch.load(args.mask_path)
        else:
            mask = None

        print("Loading checkpoints {}".format(args.ckpt_folder))

        # Use optimized model setup
        ckpt_path = os.path.join(args.ckpt_folder, "ckpts/ckpt.pth")
        model, optimizer, ema_helper, _ = self._setup_model_optimized(config, ckpt_path)
        
        criteria = torch.nn.MSELoss()

        model.train()
        start_time = time.time()
        for step in range(0, self.config.training.n_iters):
            model.train()

            # remain stage
            remain_x, remain_c = next(D_remain_iter)
            n = remain_x.size(0)
            remain_x = remain_x.to(self.device)
            remain_x = data_transform(self.config, remain_x)
            e = torch.randn_like(remain_x)
            b = self.betas

            t = torch.randint(low=0, high=self.num_timesteps, size=(n // 2 + 1,)).to(self.device)
            t = torch.cat([t, self.num_timesteps - t - 1], dim=0)[:n]
            remain_loss = loss_registry_conditional[config.model.type](model, remain_x, t, remain_c, e, b)

            # forget stage
            forget_x, forget_c = next(D_forget_iter)

            n = forget_x.size(0)
            forget_x = forget_x.to(self.device)
            forget_x = data_transform(self.config, forget_x)
            e = torch.randn_like(forget_x)
            b = self.betas

            t = torch.randint(low=0, high=self.num_timesteps, size=(n // 2 + 1,)).to(self.device)
            t = torch.cat([t, self.num_timesteps - t - 1], dim=0)[:n]

            if args.method == "ga":
                forget_loss = -loss_registry_conditional[config.model.type](model, forget_x, t, forget_c, e, b)
            else:
                a = (1 - b).cumprod(dim=0).index_select(0, t).view(-1, 1, 1, 1)
                forget_x = forget_x * a.sqrt() + e * (1.0 - a).sqrt()

                output = model(forget_x, t.float(), forget_c, mode="train")

                if args.method == "rl":
                    pseudo_c = torch.full(forget_c.shape, (args.label_to_forget + 1) % 10, device=forget_c.device)
                    pseudo = model(forget_x, t.float(), pseudo_c, mode="train").detach()
                    forget_loss = criteria(pseudo, output)

            loss = forget_loss + args.alpha * remain_loss

            if (step + 1) % self.config.training.log_freq == 0:
                end_time = time.time()
                step_time = end_time - start_time
                logging.info(f"step: {step + 1}, loss: {loss.item():.3f}, time: {step_time:.2f}s")
                start_time = time.time()

            optimizer.zero_grad()
            loss.backward()

            try:
                torch.nn.utils.clip_grad_norm_(model.parameters(), config.optim.grad_clip)
            except Exception:
                pass

            if mask:
                for name, param in model.named_parameters():
                    if param.grad is not None:
                        param.grad *= mask[name].to(param.grad.device)
            optimizer.step()

            if ema_helper:
                ema_helper.update(model)

            if (step + 1) % self.config.training.snapshot_freq == 0:
                states = [model.state_dict(), optimizer.state_dict(), step + 1]
                if ema_helper:
                    states.append(ema_helper.state_dict())

                torch.save(states, os.path.join(self.config.ckpt_dir, "ckpt.pth"))

    def load_ema_model(self):
        model = Conditional_Model(self.config)
        states = torch.load(
            os.path.join(self.args.ckpt_folder, "ckpts/ckpt.pth"),
            map_location=self.device,
        )
        model = model.to(self.device)
        model = torch.nn.DataParallel(model)
        
        # Use cleaned state dict
        cleaned_state_dict = self._clean_state_dict(states[0])
        model.load_state_dict(cleaned_state_dict, strict=True)

        if self.config.model.ema:
            ema_helper = EMAHelper(mu=self.config.model.ema_rate)
            ema_helper.register(model)
            ema_helper.load_state_dict(states[-1])
            test_model = ema_helper.ema_copy(model)
        else:
            ema_helper = None

        model.eval()
        return model

    def sample(self):
        model = Conditional_Model(self.config)
        states = torch.load(
            os.path.join(self.args.ckpt_folder, "ckpts/ckpt.pth"),
            map_location=self.device,
        )

        model = model.to(self.device)
        model = torch.nn.DataParallel(model)
        
        # Use cleaned state dict
        cleaned_state_dict = self._clean_state_dict(states[0])
        model.load_state_dict(cleaned_state_dict, strict=True)

        if self.config.model.ema:
            ema_helper = EMAHelper(mu=self.config.model.ema_rate)
            ema_helper.register(model)
            ema_helper.load_state_dict(states[-1])
            test_model = ema_helper.ema_copy(model)
        else:
            ema_helper = None

        model.eval()
        test_model = locals().get("test_model", model)

        if self.args.mode == "sample_fid":
            self.sample_fid(test_model, self.args.cond_scale)
        elif self.args.mode == "sample_classes":
            self.sample_classes(test_model, self.args.cond_scale)
        elif self.args.mode == "visualization":
            self.sample_visualization(model, str(self.args.cond_scale), self.args.cond_scale)

    def sample_classes(self, model, cond_scale):
        config = self.config
        args = self.args
        sample_dir = os.path.join(args.ckpt_folder, "class_samples")
        os.makedirs(sample_dir, exist_ok=True)
        img_id = 0
        classes, _ = create_class_labels(args.classes_to_generate, n_classes=config.data.n_classes)
        n_samples_per_class = args.n_samples_per_class

        for i in classes:
            os.makedirs(os.path.join(sample_dir, str(i)), exist_ok=True)
            if n_samples_per_class % config.sampling.batch_size == 0:
                n_rounds = n_samples_per_class // config.sampling.batch_size
            else:
                n_rounds = n_samples_per_class // config.sampling.batch_size + 1
            n_left = n_samples_per_class

            with torch.no_grad():
                for j in tqdm.tqdm(range(n_rounds), desc=f"Generating image samples for class {i} to use as dataset"):
                    if n_left >= config.sampling.batch_size:
                        n = config.sampling.batch_size
                    else:
                        n = n_left

                    x = torch.randn(n, config.data.channels, config.data.image_size, config.data.image_size, device=self.device)
                    c = torch.ones(x.size(0), device=self.device, dtype=int) * int(i)
                    x = self.sample_image(x, model, c, cond_scale)
                    x = inverse_data_transform(config, x)

                    for k in range(n):
                        tvu.save_image(x[k], os.path.join(sample_dir, str(c[k].item()), f"{img_id}.png"), normalize=True)
                        img_id += 1

                    n_left -= n

    def sample_one_class(self, model, cond_scale, class_label):
        config = self.config
        args = self.args
        sample_dir = os.path.join(args.ckpt_folder, "class_" + str(class_label))
        os.makedirs(sample_dir, exist_ok=True)
        img_id = 0
        total_n_samples = 500

        if total_n_samples % config.sampling.batch_size == 0:
            n_rounds = total_n_samples // config.sampling.batch_size
        else:
            n_rounds = total_n_samples // config.sampling.batch_size + 1
        n_left = total_n_samples

        with torch.no_grad():
            for j in tqdm.tqdm(range(n_rounds), desc=f"Generating image samples for class {class_label}"):
                if n_left >= config.sampling.batch_size:
                    n = config.sampling.batch_size
                else:
                    n = n_left

                x = torch.randn(n, config.data.channels, config.data.image_size, config.data.image_size, device=self.device)
                c = torch.ones(x.size(0), device=self.device, dtype=int) * class_label
                x = self.sample_image(x, model, c, cond_scale)
                x = inverse_data_transform(config, x)

                for k in range(n):
                    tvu.save_image(x[k], os.path.join(sample_dir, f"{img_id}.png"), normalize=True)
                    img_id += 1

                n_left -= n

    def sample_fid(self, model, cond_scale):
        config = self.config
        args = self.args
        img_id = 0

        classes, excluded_classes = create_class_labels(args.classes_to_generate, n_classes=config.data.n_classes)
        n_samples_per_class = args.n_samples_per_class

        sample_dir = f"fid_samples_guidance_{args.cond_scale}"
        if excluded_classes:
            excluded_classes_str = "_".join(str(i) for i in excluded_classes)
            sample_dir = f"{sample_dir}_excluded_class_{excluded_classes_str}"
        sample_dir = os.path.join(args.ckpt_folder, sample_dir)
        os.makedirs(sample_dir, exist_ok=True)

        for i in classes:
            if n_samples_per_class % config.sampling.batch_size == 0:
                n_rounds = n_samples_per_class // config.sampling.batch_size
            else:
                n_rounds = n_samples_per_class // config.sampling.batch_size + 1
            n_left = n_samples_per_class

            with torch.no_grad():
                for j in tqdm.tqdm(range(n_rounds), desc=f"Generating image samples for class {i} for FID"):
                    if n_left >= config.sampling.batch_size:
                        n = config.sampling.batch_size
                    else:
                        n = n_left

                    x = torch.randn(n, config.data.channels, config.data.image_size, config.data.image_size, device=self.device)
                    c = torch.ones(x.size(0), device=self.device, dtype=int) * int(i)
                    x = self.sample_image(x, model, c, cond_scale)
                    x = inverse_data_transform(config, x)

                    for k in range(n):
                        tvu.save_image(x[k], os.path.join(sample_dir, f"{img_id}.png"), normalize=True)
                        img_id += 1

                    n_left -= n

    def sample_image(self, x, model, c, cond_scale, last=True):
        try:
            skip = self.args.skip
        except Exception:
            skip = 1

        if self.args.sample_type == "generalized":
            if self.args.skip_type == "uniform":
                skip = self.num_timesteps // self.args.timesteps
                seq = range(0, self.num_timesteps, skip)
            elif self.args.skip_type == "quad":
                seq = (np.linspace(0, np.sqrt(self.num_timesteps * 0.8), self.args.timesteps) ** 2)
                seq = [int(s) for s in list(seq)]
            else:
                raise NotImplementedError
            from functions.denoising import generalized_steps_conditional

            xs = generalized_steps_conditional(x, c, seq, model, self.betas, cond_scale, eta=self.args.eta)
            x = xs
        elif self.args.sample_type == "ddpm_noisy":
            if self.args.skip_type == "uniform":
                skip = self.num_timesteps // self.args.timesteps
                seq = range(0, self.num_timesteps, skip)
            elif self.args.skip_type == "quad":
                seq = (np.linspace(0, np.sqrt(self.num_timesteps * 0.8), self.args.timesteps) ** 2)
                seq = [int(s) for s in list(seq)]
            else:
                raise NotImplementedError
            from functions.denoising import ddpm_steps_conditional

            xs = ddpm_steps_conditional(x, c, seq, model, self.betas, cond_scale)
            x = xs
        else:
            raise NotImplementedError
        if last:
            x = x[0][-1]
        return x

    def sample_visualization(self, model, step, cond_scale):
        config = self.config
        args = self.args
        n_samples = config.data.n_classes
        image_folder = os.path.join(self.config.image_dir, str(step))
        os.makedirs(image_folder, exist_ok=True)

        classes, _ = create_class_labels(args.classes_to_generate, n_classes=config.data.n_classes)

        for i in classes:
            with torch.no_grad():
                x = torch.randn(n_samples, config.data.channels, config.data.image_size, config.data.image_size, device=self.device)
                c = torch.ones(x.size(0), device=self.device, dtype=int) * int(i)
                x = self.sample_image(x, model, c, cond_scale)
                x = inverse_data_transform(config, x)
                grid = tvu.make_grid(x, nrow=n_samples, normalize=True)
                tvu.save_image(grid, os.path.join(image_folder, "image_class_{}.png".format(i)))
