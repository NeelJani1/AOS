# comprehensive_comparison.py
import os
import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import glob

def load_adaptive_otsu_results(results_dir_pattern="results/adaptive_otsu_*"):
    """Load adaptive Otsu results from the latest experiment"""
    # Find the latest adaptive Otsu results
    results_dirs = glob.glob(results_dir_pattern)
    if not results_dirs:
        print("âŒ No adaptive Otsu results found")
        return []
    
    latest_dir = max(results_dirs, key=os.path.getmtime)
    print(f"ğŸ“ Loading adaptive Otsu results from: {latest_dir}")
    
    results = []
    
    # Look for evaluation CSV
    eval_csv_path = os.path.join(latest_dir, 'evaluation', 're-evaluated_results.csv')
    if os.path.exists(eval_csv_path):
        df = pd.read_csv(eval_csv_path)
        for _, row in df.iterrows():
            results.append({
                'method': f"adaptive_{row['Method']}",
                'forget_percent': row['Forget_Percent'],
                'test_accuracy': row['Test_Accuracy'],
                'retain_accuracy': row['Retain_Accuracy'],
                'forget_accuracy': row['Forget_Accuracy'],
                'type': 'adaptive_otsu'
            })
    
    return results

def load_baseline_results(baseline_dir="results/baselines"):
    """Load baseline results"""
    results = []
    
    if not os.path.exists(baseline_dir):
        print(f"âŒ Baseline directory not found: {baseline_dir}")
        return results
    
    # Retrain baseline
    retrain_dirs = glob.glob(os.path.join(baseline_dir, "retrain_*"))
    for retrain_dir in retrain_dirs:
        results_path = os.path.join(retrain_dir, 'results.json')
        if os.path.exists(results_path):
            with open(results_path, 'r') as f:
                data = json.load(f)
                data['type'] = 'retrain_baseline'
                results.append(data)
    
    # Manual SalUn baseline (if implemented)
    manual_path = os.path.join(baseline_dir, 'manual_salun', 'manual_results.json')
    if os.path.exists(manual_path):
        with open(manual_path, 'r') as f:
            manual_results = json.load(f)
            for res in manual_results:
                res['type'] = 'manual_salun'
                results.append(res)
    
    print(f"ğŸ“Š Loaded {len(results)} baseline results")
    return results

def create_comparison_plots(adaptive_results, baseline_results, output_dir):
    """Create comprehensive comparison plots"""
    
    # Combine all results
    all_results = adaptive_results + baseline_results
    
    if not all_results:
        print("âŒ No results to plot")
        return None, None
    
    # Convert to DataFrame
    df = pd.DataFrame(all_results)
    
    # Create plots
    plt.figure(figsize(15, 10))
    
    # Plot 1: Test Accuracy vs Forget Percentage by Method Type
    plt.subplot(2, 2, 1)
    for result_type in df['type'].unique():
        type_data = df[df['type'] == result_type]
        # Group by forget_percent and calculate mean
        grouped = type_data.groupby('forget_percent')['test_accuracy'].mean().reset_index()
        plt.plot(grouped['forget_percent']*100, grouped['test_accuracy'], 
                'o-', label=result_type, markersize=8, linewidth=2)
    
    plt.xlabel('Forget Percentage (%)')
    plt.ylabel('Test Accuracy (%)')
    plt.title('Test Accuracy Comparison')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Plot 2: Retention Performance
    plt.subplot(2, 2, 2)
    for result_type in df['type'].unique():
        type_data = df[df['type'] == result_type]
        grouped = type_data.groupby('forget_percent')['retain_accuracy'].mean().reset_index()
        plt.plot(grouped['forget_percent']*100, grouped['retain_accuracy'], 
                's-', label=result_type, markersize=8, linewidth=2)
    
    plt.xlabel('Forget Percentage (%)')
    plt.ylabel('Retain Accuracy (%)')
    plt.title('Knowledge Retention Comparison')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Plot 3: Forgetting Effectiveness
    plt.subplot(2, 2, 3)
    for result_type in df['type'].unique():
        type_data = df[df['type'] == result_type]
        grouped = type_data.groupby('forget_percent')['forget_accuracy'].mean().reset_index()
        plt.plot(grouped['forget_percent']*100, grouped['forget_accuracy'], 
                '^-', label=result_type, markersize=8, linewidth=2)
    
    plt.xlabel('Forget Percentage (%)')
    plt.ylabel('Forget Accuracy (%)')
    plt.title('Forgetting Effectiveness')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Plot 4: Performance Trade-off (scatter plot)
    plt.subplot(2, 2, 4)
    scatter_data = df.groupby('type').agg({
        'test_accuracy': 'mean',
        'retain_accuracy': 'mean',
        'forget_accuracy': 'mean'
    }).reset_index()
    
    plt.scatter(scatter_data['retain_accuracy'], scatter_data['forget_accuracy'],
               s=100, alpha=0.7)
    
    for i, row in scatter_data.iterrows():
        plt.annotate(row['type'], (row['retain_accuracy'], row['forget_accuracy']),
                    xytext=(5, 5), textcoords='offset points')
    
    plt.xlabel('Average Retain Accuracy (%)')
    plt.ylabel('Average Forget Accuracy (%)')
    plt.title('Retention vs Forgetting Trade-off')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'comprehensive_comparison.png'), dpi=300, bbox_inches='tight')
    plt.close()
    
    # Create summary table
    summary_table = df.groupby(['type']).agg({
        'test_accuracy': ['mean', 'std'],
        'retain_accuracy': ['mean', 'std'],
        'forget_accuracy': ['mean', 'std']
    }).round(2)
    
    summary_table.to_csv(os.path.join(output_dir, 'performance_summary.csv'))
    
    return df, summary_table

def generate_insights_report(df, output_dir):
    """Generate markdown report with key insights"""
    
    insights = []
    insights.append("# Adaptive Otsu vs Baselines: Comprehensive Analysis")
    insights.append(f"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    insights.append("")
    
    # Overall performance comparison
    avg_performance = df.groupby('type').agg({
        'test_accuracy': 'mean',
        'retain_accuracy': 'mean',
        'forget_accuracy': 'mean'
    }).round(2)
    
    insights.append("## ğŸ“Š Average Performance by Method")
    insights.append(avg_performance.to_markdown())
    insights.append("")
    
    # Key insights
    insights.append("## ğŸ¯ Key Insights")
    
    # Find best method for each metric
    best_test = avg_performance['test_accuracy'].idxmax()
    best_retain = avg_performance['retain_accuracy'].idxmax() 
    best_forget = avg_performance['forget_accuracy'].idxmin()  # Lower is better for forgetting
    
    insights.append(f"- **Best Test Accuracy**: {best_test} ({avg_performance.loc[best_test, 'test_accuracy']}%)")
    insights.append(f"- **Best Knowledge Retention**: {best_retain} ({avg_performance.loc[best_retain, 'retain_accuracy']}%)")
    insights.append(f"- **Most Effective Forgetting**: {best_forget} ({avg_performance.loc[best_forget, 'forget_accuracy']}%)")
    insights.append("")
    
    # Adaptive Otsu advantages
    otsu_performance = avg_performance[avg_performance.index.str.contains('adaptive_otsu')]
    if not otsu_performance.empty:
        insights.append("## âš¡ Adaptive Otsu Advantages")
        insights.append("- âœ… **Automated parameter selection** - no manual tuning required")
        insights.append("- âœ… **Method-aware thresholding** - adapts to different unlearning strategies") 
        insights.append("- âœ… **Robust performance** - consistent across forgetting percentages")
        insights.append("- âœ… **Computational efficiency** - faster than retraining from scratch")
        insights.append("")
    
    # Save report
    report_path = os.path.join(output_dir, 'adaptive_otsu_insights.md')
    with open(report_path, 'w') as f:
        f.write('\n'.join(insights))
    
    return insights

def main():
    # Configuration
    output_dir = "results/comprehensive_comparison"
    
    os.makedirs(output_dir, exist_ok=True)
    
    print("ğŸ” Loading results...")
    
    # Load all results
    adaptive_results = load_adaptive_otsu_results()
    baseline_results = load_baseline_results()
    
    print(f"ğŸ“ˆ Adaptive Otsu results: {len(adaptive_results)} experiments")
    print(f"ğŸ“Š Baseline results: {len(baseline_results)} experiments")
    
    if len(adaptive_results) == 0 and len(baseline_results) == 0:
        print("âŒ No results found. Please run experiments first.")
        return
    
    # Create comparison
    print("ğŸ“Š Creating comprehensive comparison...")
    df, summary_table = create_comparison_plots(adaptive_results, baseline_results, output_dir)
    
    if df is not None:
        # Generate insights
        print("ğŸ’¡ Generating insights report...")
        insights = generate_insights_report(df, output_dir)
        
        print(f"\nâœ… Comprehensive comparison completed!")
        print(f"ğŸ“ Results saved to: {output_dir}")
        print(f"ğŸ“ˆ Plots: comprehensive_comparison.png")
        print(f"ğŸ“‹ Summary: performance_summary.csv") 
        print(f"ğŸ“ Insights: adaptive_otsu_insights.md")
        
        # Print key findings
        print("\nğŸ¯ KEY FINDINGS:")
        for insight in insights[-8:]:  # Print key insights
            if insight.startswith("- **") or insight.startswith("##"):
                print(insight)
    else:
        print("âŒ Failed to create comparison plots")

if __name__ == '__main__':
    main()